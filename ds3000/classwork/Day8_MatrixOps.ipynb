{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DS3000 Day 8\n",
    "\n",
    "Oct 11, 2024\n",
    "\n",
    "Admin\n",
    "- Lab 2 Grades (and Solutions!) Posted. **Please** check the solutions and compare with your own before submitting a regrade request.\n",
    "- Take-Home Coding Exam due **Sunday, Oct. 13, by 11:59 pm**\n",
    "- There will be a virtual visitor in Dr. Gerber's Tuesday section (**11:45 am, INV 019**) all students are welcome to come either in-person or attend via [Zoom (Link)](https://northeastern.zoom.us/j/96399994252). The visitor is Alumni Justin Chen, who graduated last year and works for a **major** Baseball team.\n",
    "- Meet with assigned TA by next **Friday, Oct. 18** (all students *must* attend the meeting)\n",
    "- Phase II of the project due **Friday, Oct. 25** (Group submission to Gradescope, see ProjectGuidelines on Canvas)\n",
    "- Homework 3 due **Tuesday, Oct. 29**\n",
    "\n",
    "Push-Up Tracker\n",
    "- Section 03: 4\n",
    "- Section 05: 2\n",
    "\n",
    "Content:\n",
    "- Linear Algebra Basics Continued...\n",
    "  - By Hand and In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages for today\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Overview\n",
    "\n",
    "All data needs to be represented mathematically in order to perform Machine Learning. What is *Machine Learning* anyway?\n",
    "\n",
    "- Start with data: observed measures from the real world, store them as vectors/matrices\n",
    "- Develop a strategy of analysis that leads to choosing an appropriate algorithm\n",
    "- \"Learn\" $\\rightarrow$ improve the algorithm, identify patterns in the data by performing mathematical operations on the vectors/matrices\n",
    "- Make predictions on new observations based on the results of the model, or clarify understanding of patterns\n",
    "\n",
    "### Example ML Algorithms\n",
    "\n",
    "Here are some example ML algorithms, some of which we'll be studying. They all rely on the mathematical concepts we'll be covering over the next several classes (as you'll see).\n",
    "\n",
    "#### Supervised Learning (Prediction)\n",
    "- Linear*, Multiple*, Polynomial* Regression (Regression: predicting 1 numeric feature with 1 or more numeric/categorical features)\n",
    "- Linear Perceptron* (Classification: predicting 1 categorical feature with 2 or more numeric/categorical features)\n",
    "- Gradient Descent* and Neural Networks (Feed-Forward*) (Regression or Classification)\n",
    "- Logistic Regression**\n",
    "- Nearest Neighbor Classifiers**\n",
    "- Decision Trees, Random Forests**\n",
    "- Support Vector Machines\n",
    "- Bayesian Models\n",
    "\n",
    "#### Unsupervised Learning (Inference)\n",
    "- Principal Component Analysis* (Dimensionality Reduction: Understanding how numeric features relate to each other)\n",
    "- K-means, Clustering**\n",
    "- Factor Analysis\n",
    "- Gaussian Mixture Models\n",
    "- Kernel Estimation\n",
    "\n",
    "#### Key\n",
    "- \"*\" means we will be (or hope/plan to) cover it in this course\n",
    "- \"**\" means we'll have talked about the basic mathematical concepts that make these methods up, and are simple enough that I believe you could learn it on your own outside of class (and potentially apply it in a project setting)\n",
    "- The rest are not too terribly difficult, though perhaps would require a bit more effort on your part to figure out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Time: Linear Algebra Basics\n",
    "## Vector Geometry (on board and in python)\n",
    "\n",
    "Previously, we looked at representing data as vectors, and already talked about calculating distances between vectors such as  $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ and $x' = \\begin{bmatrix} x_1' \\\\ x_2' \\end{bmatrix}$. Data are usually represented as vectors (or matrices). For example, from the penguin data:\n",
    "\n",
    "$x_0 = \\begin{bmatrix} -0.895341 \\\\ 0.778650 \\\\ -1.428247 \\\\ -0.567127 \\end{bmatrix}$\n",
    "\n",
    "Which means that the first penguin had above average bill depth, but below average everything else. In python, we usually store vectors using NumPy arrays (let's round to three decimals to make it look cleaner):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm_scaled</th>\n",
       "      <th>bill_depth_mm_scaled</th>\n",
       "      <th>flipper_length_mm_scaled</th>\n",
       "      <th>body_mass_g_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.894695</td>\n",
       "      <td>0.779559</td>\n",
       "      <td>-1.424608</td>\n",
       "      <td>-0.567621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.821552</td>\n",
       "      <td>0.119404</td>\n",
       "      <td>-1.067867</td>\n",
       "      <td>-0.505525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.675264</td>\n",
       "      <td>0.424091</td>\n",
       "      <td>-0.425733</td>\n",
       "      <td>-1.188572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.333559</td>\n",
       "      <td>1.084246</td>\n",
       "      <td>-0.568429</td>\n",
       "      <td>-0.940192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.858123</td>\n",
       "      <td>1.744400</td>\n",
       "      <td>-0.782474</td>\n",
       "      <td>-0.691811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm_scaled  bill_depth_mm_scaled  flipper_length_mm_scaled  \\\n",
       "0              -0.894695              0.779559                 -1.424608   \n",
       "1              -0.821552              0.119404                 -1.067867   \n",
       "2              -0.675264              0.424091                 -0.425733   \n",
       "4              -1.333559              1.084246                 -0.568429   \n",
       "5              -0.858123              1.744400                 -0.782474   \n",
       "\n",
       "   body_mass_g_scaled  \n",
       "0           -0.567621  \n",
       "1           -0.505525  \n",
       "2           -1.188572  \n",
       "4           -0.940192  \n",
       "5           -0.691811  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_penguin = sns.load_dataset('penguins')\n",
    "df_penguin.dropna(axis=0, inplace=True)\n",
    "# only focus on numerical features (for now)\n",
    "col_num_list = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "df_penguin_num = df_penguin.loc[:, col_num_list]\n",
    "# standardize\n",
    "# subtracting the mean and dividing each feature by the standard deviation (standardization)\n",
    "df_penguin_num_scaled = pd.DataFrame()\n",
    "for feat in df_penguin_num.columns:\n",
    "    df_penguin_num_scaled[f'{feat}_scaled'] = (df_penguin_num[feat] - df_penguin_num[feat].mean()) / df_penguin_num[feat].std()\n",
    "\n",
    "df_penguin_num_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.895,  0.78 , -1.425, -0.568])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_penguin0 = np.array(df_penguin_num_scaled.iloc[0, :]).round(3)\n",
    "vec_penguin0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation\n",
    "\n",
    "Generally, we use **lowercase** letters to represent vectors, (for example, instead of writing out \"Penguin 0\", we would call the Austria vector $\\vec{x}_0$, and **uppercase** letters to represent matrices, such as when we consider the data set containing the first two penguins (standardized):\n",
    "\n",
    "$X = \\begin{bmatrix} -0.895 & 0.779 & -1.428 & -0.567 \\\\ -0.822 & 0.119 & -1.068 & -0.506 \\end{bmatrix}$\n",
    "\n",
    "Note that by convention also, vectors are **column vectors**, but that when we combine vectors into a data matrix, the vectors are included as the **rows**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Operations (in short)\n",
    "### Vector Addition\n",
    "\n",
    "In order to add two vectors together, they must be the same **dimension**. For example, if each vector $\\vec{x}_i \\in \\mathbb{R}^2$):\n",
    "\n",
    "$\\vec{x}_1 = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$\n",
    "$\\vec{x}_2 = \\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "$\\vec{x}_1 + \\vec{x}_2 = \\begin{bmatrix} 10 \\\\ 5 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([3, 4])\n",
    "x2 = np.array([7, 1])\n",
    "x1 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector \"Multiplication\"\n",
    "\n",
    "You can easily multiply scalars (real numbers) to vectors:\n",
    "\n",
    "$c\\vec{x}_1 = \\begin{bmatrix} 3(c) \\\\ 4(c) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar multiplication (what would 3 of penguin 1 look like?)\n",
    "c=3\n",
    "c*x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we talk about \"multiplying\" vectors together, there may be more than one interpretation of that. When we think of multipying matrices elementwise, this is called the **Hadamard product**:\n",
    "\n",
    "$\\vec{x}_1 = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$\n",
    "$\\vec{x}_2 = \\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "$\\vec{x}_1 \\odot \\vec{x}_2 = \\begin{bmatrix} 21 \\\\ 4 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21,  4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 * x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common (and useful) operation is the **dot product**. For two vectors $x$ and $y$, the dot product is:\n",
    "\n",
    "$x\\cdot y = \\sum_i x_i \\times y_i$\n",
    "\n",
    "That is, the sum of all the pairwise products of the vectors (or, equivalently, the sum of the Hadamard product vector). Let's use a simple, two-dimensional example instead of our countries for a moment:\n",
    "\n",
    "$x = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$\n",
    "\n",
    "$y = \\begin{bmatrix} 7 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "$x \\cdot y = 3\\times7 + 4\\times1 = 21 + 4 = 25$\n",
    "\n",
    "This is a common operation in ML that we'll use quite a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([3, 4])\n",
    "y = np.array([7, 1])\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful operation we might use on occasion is finding the length of a vector:\n",
    "\n",
    "$||x|| = \\sqrt{\\sum_i x_i^2}$\n",
    "\n",
    "For example, if:\n",
    "\n",
    "$x = \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix}$\n",
    "$||x|| = \\sqrt{3^2  + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$\n",
    "\n",
    "We can also use graphing to understand this a little bit better (professor will show this on the board)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in numpy\n",
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also called finding the $\\ell 2$-**norm** of the vector. There are other **norms** which we may talk about if necessary as we proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data as Matrices\n",
    "\n",
    "We've already talked at length about representing data as vectors, for example:\n",
    "\n",
    "$x_0 = \\begin{bmatrix} -0.895341 \\\\ 0.778650 \\\\ -1.428247 \\\\ -0.567127 \\end{bmatrix}$\n",
    "\n",
    "Is the first penguin in the Seaborn data set. While some machine learning algorithms can deal with observations/vectors individually (on an iterative basis), many machine learning algorithms consider the observations collectively (as a matrix). This is why general practice is to treat data as **matrices**:\n",
    "\n",
    "$$X = \\begin{bmatrix} -0.895 & 0.779 & -1.428 & -0.567 \\\\ -0.822 & 0.119 & -1.068 & -0.506 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\end{bmatrix}$$\n",
    "\n",
    "where each row of the matrix is an observation, and each column a feature. Many of the common operations that make up machine learning algorithms require treating data as matrices. It is important also to remember that, in fact, *vectors **are** matrices*, they are simply matrices with one of the dimensionalities equal to one. For example:\n",
    "\n",
    "$$\\vec{a} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix}$$\n",
    "\n",
    "Is both a 3-dimensional vector and a $3 \\times 1$ matrix. It's transpose (flipping the rows and columns): $\\vec{a}^T = \\begin{bmatrix} a_1 & a_2 & a_3 \\end{bmatrix}$ is both a 3-dimensional vector and a $1 \\times 3$ matrix. The below:\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ -4 & -5 & -6 \\end{bmatrix}$$\n",
    "$$B = \\begin{bmatrix} -5 & 6 & 1 & -2 \\\\ 0 & 1 & 1 & 0 \\\\ 8 & 6 & 4 & -2 \\end{bmatrix}$$\n",
    "\n",
    "are both matrices, and we would say that $A$ is $2 \\times 3$ and that $B$ is $3 \\times 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Math and Manipulations\n",
    "\n",
    "Assume you have two general matrices, $A$ which has shape $n \\times m$ and $B$ which has shape $p \\times q$. Some shapes are compatible for matrix multiplication, and many are not:\n",
    "\n",
    "- The **inner dimensions** must match to multiply matrices\n",
    "  - i.e. you may multiply a $2 \\times 3$ and a $3 \\times 4$ matrix. You may *not* multiply a $2 \\times 3$ and $2 \\times 3$ matrix.\n",
    "- **Order matters**\n",
    "  - based on the above, you should note that if $A$ is $2 \\times 3$ and $B$ is $3 \\times 4$, you may find $AB$ but **NOT** $BA$.\n",
    " \n",
    "**Also**: matrix multiplication is **NOT** pairwise multiplication (that's the Hadamard product!). This should be obvious from the restrictions above. If you cannot multiply $2\\times3$ and $2\\times3$, then multiplication can't be that simple. So how do we do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "             [-4, -5, -6]])\n",
    "B = np.array([[-5, 6, 1, -2],\n",
    "             [0, 1, 1, 0],\n",
    "             [8, 6, 4, -2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the error when we try to multiply pairwise elements (* operator)\n",
    "# you can't take the Hadamard product of matrices of different dimension\n",
    "# A * B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 19,  26,  15,  -8],\n",
       "       [-28, -65, -33,  20]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the numpy function that does matrix multiplication\n",
    "np.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened? How does matrix multiplication work? (Demonstration on the whiteboard)\n",
    "\n",
    "Notice that the resulting matrix from multiplying $A$ by $B$ **kept the outer dimensions of the two matrices**. I.e. multiplying a $2\\times3$ matrix by a $3\\times4$ matrix resulted in a single $2\\times4$ matrix. This is because matrix multiplication is a result of:\n",
    "\n",
    "- Each element in the product matrix is the **dot product** of the corresponding **row from the left matrix** and **column from the right matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first row, first column\n",
    "np.dot(A[0,:], B[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first row, second column\n",
    "np.dot(A[0,:], B[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second row, first column\n",
    "np.dot(A[1,:], B[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, if vectors are matrices...\n",
    "\n",
    "As long as the inner dimensions match, we can multiply matrices. This means we can multiply vectors by matrices and matrices by vectors. In fact:\n",
    "\n",
    "- Matrix-vector multiplcation ($A\\vec{x}$, for matrix $A$ and vector $\\vec{x}$) is a linear combination of the **rows** of $A$\n",
    "- Vector-Matrix multiplcation ($\\vec{x}A$, for vector $\\vec{x}$ and matrix $A$) is a linear combination of the **columns** of $A$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ -4 & -5 & -6 \\end{bmatrix}$$\n",
    "$$\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\\\ -2 \\end{bmatrix}$$\n",
    "$$A\\vec{x} = \\begin{bmatrix} 1 & 2 & 3 \\\\ -4 & -5 & -6 \\end{bmatrix}  \\begin{bmatrix} 2 \\\\ 4 \\\\ -2 \\end{bmatrix} = \\begin{bmatrix} 1(2) + 2(4) + 3(-2) \\\\ (-4)(2) + (-5)(4) + (-6)(-2) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ -16 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4, -16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([2, 4, -2])\n",
    "np.matmul(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that order matters; we cannot do xA because x is 3x1 and A is 2x3:\n",
    "#np.matmul(x, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think for a moment about the structure of our penguin data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_penguin_num_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **dimension** of the matrix that represents all our data; 333 countries with 4 numeric features measured about each one. Since we're going to be treating it as a matrix, it would make sense to cast it to an **array** in Python using NumPy. Note that you can't do this if any of the columns of your data are strings/categorical, because arrays, just like matrices, can only have numbers in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.89469547,  0.77955895, -1.42460769, -0.56762058],\n",
       "       [-0.82155152,  0.11940428, -1.06786655, -0.50552542],\n",
       "       [-0.67526362,  0.42409105, -0.42573251, -1.18857213],\n",
       "       ...,\n",
       "       [ 1.1716211 , -0.74387492,  1.50066961,  1.91618562],\n",
       "       [ 0.22074976, -1.20090508,  0.78718734,  1.23313892],\n",
       "       [ 1.08019116, -0.5407504 ,  0.85853557,  1.48151954]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdat = np.array(df_penguin_num_scaled)\n",
    "Xdat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector Multiplication as a Transformation\n",
    "\n",
    "Let us focus only on the situation where we have a matrix multiplied by a column vector, as above, but let's simplify (for now) to the situation where the matrix $A$ is a **square** matrix (i.e. same number of rows and columns). This means that the vector $\\vec{x}$ must have the same number of rows as $A$, and the resulting product $A\\vec{x}$ will be of the same dimension as $\\vec{x}$:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$$\n",
    "$$\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$A\\vec{x} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 2(2) + 0(4) \\\\ 0(2) + 3(4) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 12 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 12])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[2,0],\n",
    "              [0,3]])\n",
    "x = np.array([2, 4])\n",
    "np.matmul(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we think about this in terms of **how $A$ *changes* $\\vec{x}$**? Before applying the matrix $A$ to $\\vec{x}$, it was $\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$, and afterwards it was $A\\vec{x} = \\begin{bmatrix} 4 \\\\ 12 \\end{bmatrix}$, which we note is a vector of the same dimension. You can think of this graphically as $A$ **transforming** $\\vec{x}$.\n",
    "\n",
    "(Draw a picture on the board)\n",
    "\n",
    "Now what happens if we apply a different matrix, $B$? Perhaps something like:\n",
    "\n",
    "$$B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$$\n",
    "$$\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$B\\vec{x} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 0(2) + (-1)(4) \\\\ 1(2) + 0(4) \\end{bmatrix} = \\begin{bmatrix} -4 \\\\ 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4,  2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[0,-1],\n",
    "              [1,0]])\n",
    "np.matmul(B, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Draw a picture on the board)\n",
    "\n",
    "The values in the **diagonal** elements of $A$ seem to **scale** the vector $\\vec{x}$ when applied, and the values in the **off-diagonal** elements of $B$ seem to **rotate** the vector $\\vec{x}$ when applied. So, if we combine them, what happens?\n",
    "\n",
    "$$A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}$$\n",
    "$$B = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$$\n",
    "$$\\vec{x} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$AB\\vec{x} = ??$$\n",
    "\n",
    "- Remember that order matters, so the above $AB\\vec{x}$ will **first rotate, then scale**, while $BA\\vec{x}$ will **first scale, then rotate**. Note that this *does* make a difference in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8,  6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(A, np.matmul(B, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-12,   4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(B, np.matmul(A, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Draw pictures on the board to illustrate)\n",
    "\n",
    "Finally note that if you tried to combine the two actions into one matrix, $C$ (by adding the matrices together), the matrix operations are not \"additive\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 14])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([[2, -1],\n",
    "              [1, 3]])\n",
    "np.matmul(C, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Spaces and Spans\n",
    "\n",
    "![d](https://i.redd.it/3b82i66pulz81.jpg)\n",
    "What is a vector space?\n",
    "\n",
    "- The coordinate planes defined by the dimensions make up the vector space; i.e. the number line makes up the 1-dimensional \"vector\" space, the $x-y$ axes make up the 2-dimensional vector space (a plane), while the $x-y-z$ axes make up the 3-dimensional vector space, etc.\n",
    "\n",
    "The **basis vectors** of a vector space are the vectors that \"define\" the direction of the axes, for example:\n",
    "\n",
    "- the $x-y$ plane has basis vectors: $\\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n",
    "  - any other vector in 2-dimensional space can be reached by a linear combination of the two basis vectors.\n",
    " \n",
    "**Example:**\n",
    "\n",
    "$$\\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} = 3\\hat{i} + 4\\hat{j}$$\n",
    "$$\\begin{bmatrix} -23 \\\\ 42 \\end{bmatrix} = -23\\hat{i} + 42\\hat{j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ihat = np.array([1,0])\n",
    "jhat = np.array([0,1])\n",
    "3*ihat + 4*jhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-23,  42])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-23*ihat + 42*jhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use *other* vectors as \"basis\" vectors. For example, you can determine which coordinates can be reached by a linear combination of the following two vectors:\n",
    "\n",
    "$$\\vec{v} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$$\n",
    "$$\\vec{w} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "This is called finding the **span** of a set of vectors. The **span of the basis vectors $\\hat{i}$ and $\\hat{j}$ is the entire $x-y$ plane**. How do you determine the span of a set of vectors? Use placeholders:\n",
    "\n",
    "$$\\alpha \\vec{v} + \\beta \\vec{w} = \\alpha \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} + \\beta \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2\\alpha + \\beta \\\\ 2\\beta \\end{bmatrix}$$\n",
    "\n",
    "This is a 2-dimensional vector where (using $x$ for the $x$-axis and $y$ for the $y$-axis), $x = 2\\alpha + \\beta$ or $y = 2\\beta$. These functions help us **define the span**; note that there are no restrictions on what $x$ and $y$ can be (given any choices of $\\alpha$ and $\\beta$), meaning that these two vectors' span is also the entire $x-y$ plane:\n",
    "\n",
    "$$y = 2(x - 2\\alpha) \\rightarrow y = 2x - 4\\alpha$$\n",
    "\n",
    "**Example (when the span is *not* the entire plane):**\n",
    "\n",
    "$$\\vec{v} = \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}$$\n",
    "$$\\vec{w} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}$$\n",
    "$$\\alpha \\vec{v} + \\beta \\vec{w} = \\alpha \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix} + \\beta \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} -\\alpha + 2\\beta \\\\ -2\\alpha + 4\\beta \\end{bmatrix}$$\n",
    "\n",
    "Which means $x = -\\alpha + 2\\beta$ and $y = -2\\alpha + 4\\beta$, which in the 2-dimensional vector space is simply the **line $y=2x$**:\n",
    "\n",
    "$$y = -2\\alpha + 4\\beta = 2(-\\alpha + 2\\beta) = 2x$$\n",
    "\n",
    "**Fact/Note:** if a 2-d vector is a multiple of the other, you are guaranteed to have a line as the span of the two vectors (above, for example, $\\vec{w} = -2\\vec{v}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spans In Summary\n",
    "\n",
    "In two-dimensions, there are three cases for the span of any set of 2-dimensional vectors:\n",
    "\n",
    "- Every point in the plane (see above)\n",
    "- A line passing through the origin (see above)\n",
    "- The origin (special case: the span of a set of origin vectors)\n",
    "\n",
    "In N-dimensions, there are *still* three cases for the span of any set of $N$-dimensional vectors:\n",
    "\n",
    "- Every point in the $N$-dimensional space\n",
    "- A reduced dimensionality space, passing through the origin (e.g. a plane or a line in 3-dimensions)\n",
    "- The origin\n",
    "\n",
    "**Finally**: the span of $N$ vectors is never more than $N$-dimensional space.\n",
    "\n",
    "- Example: The span of any single vector (of any dimension) is either a line or the origin (if it is the origin):\n",
    "\n",
    "$$\\vec{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$$\n",
    "\n",
    "Even though $\\vec{v}$ exists in 3-dimensional space, the span of this single vector are any points reached by: $\\alpha \\vec{v} = \\begin{bmatrix} \\alpha \\\\ 2\\alpha \\\\ 3\\alpha \\end{bmatrix}$, which is the line $z = x + y$ in 3-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Dependence and Independence\n",
    "\n",
    "A set of vectors is **linearly dependent** if one of the vectors is a linear combination of the others:\n",
    "\n",
    "- i.e. if the span is a line (see above, and below)\n",
    "\n",
    "A set of vectors is **linearly independent** if each vector adds a new dimension to the span\n",
    "\n",
    "- see below for general idea\n",
    "\n",
    "**Linearly Dependent Vectors**:\n",
    "\n",
    "The set of vectors: $\\vec{a} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\vec{b} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, and $\\vec{c} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 0 \\end{bmatrix}$ is linearly dependent because $\\vec{c} = 2\\vec{a} + 3\\vec{b}$.\n",
    "\n",
    "**Linearly Independent Vectors**:\n",
    "\n",
    "The set of vectors: $\\vec{a} = \\begin{bmatrix} \\alpha \\\\ 0 \\end{bmatrix}$ and $\\vec{b} = \\begin{bmatrix} \\beta \\\\ \\text{Anything Non-Zero} \\end{bmatrix}$ for any $\\alpha$ and $\\beta$ is linearly independent.\n",
    "\n",
    "**Fact/Note:** $N+1$ or more vectors of length $N$ are linearly dependent. Example:\n",
    "\n",
    "The set of vectors: $\\vec{a} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $\\vec{b} = \\begin{bmatrix} -4 \\\\ 6 \\end{bmatrix}$, and $\\vec{c} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ is linearly dependent because $\\vec{b} = -2\\vec{a} + 8\\vec{c}$.\n",
    "\n",
    "- This is actually a very important point for machine learning with data. When you have more **features** than **observations** (this is called the [Large p, small n problem](https://www.google.com/search?q=Large+p%2C+small+n+problem&sca_esv=576533920&sxsrf=AM9HkKl_8u1hxRNyO9OTf4YbeWyD68GxSw%3A1698255238798&ei=hlE5ZduOMIKU5OMPj5mxOA&ved=0ahUKEwjb6fvh3ZGCAxUCCnkGHY9MDAcQ4dUDCBA&uact=5&oq=Large+p%2C+small+n+problem&gs_lp=Egxnd3Mtd2l6LXNlcnAiGExhcmdlIHAsIHNtYWxsIG4gcHJvYmxlbTIFEAAYgAQyCBAAGIoFGIYDMggQABiKBRiGAzIIEAAYigUYhgNIqSdQ0AZYuiZwBHgBkAEBmAHsAaABpRWqAQYxNy42LjK4AQPIAQD4AQHCAgoQABhHGNYEGLADwgIEECMYJ8ICBxAuGIoFGCfCAggQABiKBRiRAsICCxAuGIoFGLEDGIMBwgIREC4YgAQYsQMYgwEYxwEY0QPCAgcQIxiKBRgnwgILEC4YgwEYsQMYgATCAgsQABiKBRixAxiDAcICCxAAGIAEGLEDGIMBwgIOEC4YgAQYxwEYrwEYjgXCAggQLhiABBixA8ICDhAuGIAEGLEDGMcBGNEDwgIIEAAYgAQYsQPCAgoQABiABBgUGIcCwgIGEAAYFhge4gMEGAAgQYgGAZAGCA&sclient=gws-wiz-serp)) it means that you are almost certainly going to overfit your data (and, that the features are linearly dependent). We can see a practical example of this when we learn line of best fit shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing In/Dependence using Reduced Row Echelon Form (RREF)\n",
    "### (Tentative: may skip depending on if we're running behind schedule or not)\n",
    "\n",
    "One quick and easy way to determine if a set of vectors are independent or not (as long as you have only as many vectors as elements and not more) is to put them into a matrix and **row reduce** that matrix. **Row reduction** involves operating on the matrix in specific/limited ways in order to put into a specific form, Reduced Row Echelon Form (RREF) which consists of:\n",
    "\n",
    "- Diagonal elements of the matrix are all 1 or 0\n",
    "- All elements above and below the 1's on the diagonal are 0\n",
    "- Any rows made up entirely of 0's are on the bottom of the matrix\n",
    "\n",
    "The operations you can perform on the matrix are limited to:\n",
    "\n",
    "- Scaling rows (i.e. multiplying rows by some non-zero constant)\n",
    "- Summing/Subtracting rows (i.e. taking one row and adding/subtracting it from another)\n",
    "- Swapping rows (self explanatory)\n",
    "\n",
    "Once you are done putting the matrix into RREF, if you have the identity matrix, the vectors are independent; if there are any 0 rows, the vectors are dependent. \n",
    "\n",
    "**Example**: We know that the set of vectors  $\\vec{a} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\vec{b} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, and $\\vec{c} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 0 \\end{bmatrix}$ is linearly dependent because $\\vec{c} = 2\\vec{a} + 3\\vec{b}$; can we show this by finding the RREF?\n",
    "\n",
    "$$\\left[\n",
    "    \\begin{array}{ccc}\n",
    "        1 & 0 & 2\\\\\n",
    "        0 & 1 & 3\\\\\n",
    "        0 & 0 & 0\n",
    "    \\end{array}\n",
    "\\right]$$\n",
    "\n",
    "Well... that was easy! In fact, the matrix is already **IN** RREF, and there is a zero row. What about a more complicated example? Let's use $\\vec{a} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 0 \\end{bmatrix}$, $\\vec{b} = \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}$, and $\\vec{c} = \\begin{bmatrix} 1 \\\\ -4 \\\\ 1 \\end{bmatrix}$:\n",
    "\n",
    "These vectors are independent! This also happens to mean that their span is all of 3-dimensional space (since, you could reach any 3-dimensional vector you'd like to with a linear combination of them). Notice that wasn't true of the first set of three vectors; if a set of vectors is dependent, the span is a lower dimensional space.\n",
    "\n",
    "$$\\left[\n",
    "    \\begin{array}{ccc}\n",
    "        1 & -1 & 1 \\\\\n",
    "        2 & 1 & -4 \\\\\n",
    "        0 & -1 & 1 \n",
    "    \\end{array}\n",
    "\\right]\n",
    "        \\xrightarrow[]{r_1'= (r_1 - 2r_0)/3}\n",
    "        \\left[\n",
    "    \\begin{array}{ccc}\n",
    "        1 & -1 & 1 \\\\\n",
    "        0 & 1 & -2 \\\\\n",
    "        0 & -1 & 1 \n",
    "    \\end{array}\n",
    "\\right]\n",
    "        \\xrightarrow[r_2'=-(r_2+r_1)]{r_0'=r_0 + r_1}\n",
    "        \\left[\n",
    "    \\begin{array}{ccc}\n",
    "        1 & 0 & -1 \\\\\n",
    "        0 & 1 & -2 \\\\\n",
    "        0 & 0 & 1 \n",
    "    \\end{array}\n",
    "\\right]\n",
    "        \\xrightarrow[r_1'=r_1 + 2 r_2]{r_0': r_0 + r_2}\n",
    "        \\left[\n",
    "    \\begin{array}{ccc}\n",
    "        1 & 0 & 0 \\\\\n",
    "        0 & 1 & 0 \\\\\n",
    "        0 & 0 & 1 \n",
    "    \\end{array}\n",
    "\\right]$$\n",
    "\n",
    "We can avoid doing the RREF by hand, and instead use the Python module `sympy` which does it very quickly for us by casting our NumPy array to a `Matrix` object from that package and using the `.rref` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sympy in c:\\users\\eager\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.12)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\eager\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0],\n",
       "[0, 1, 0],\n",
       "[0, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "M = Matrix(np.array([[1, -1, 1], [2, 1, -4], [0, -1, 1]]))\n",
    "# Use sympy.rref() method  \n",
    "M_rref = M.rref()   \n",
    "      \n",
    "M_rref[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us figure out very quickly if much larger sets of vectors are independent. For example, what about these four 4-dimensional vectors:\n",
    "\n",
    "$\\vec{a} = \\begin{bmatrix} -1 \\\\ 2 \\\\ 1 \\\\ -2 \\end{bmatrix}$, $\\vec{b} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$, $\\vec{c} = \\begin{bmatrix} -2 \\\\ 1 \\\\ 3 \\\\ 4 \\end{bmatrix}$, and $\\vec{d} = \\begin{bmatrix} -3 \\\\ 3 \\\\ 4 \\\\ 2 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-1 & 0 & -2 & -3\\\\2 & 1 & 1 & 3\\\\1 & 0 & 3 & 4\\\\-2 & 1 & 4 & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-1, 0, -2, -3],\n",
       "[ 2, 1,  1,  3],\n",
       "[ 1, 0,  3,  4],\n",
       "[-2, 1,  4,  2]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = Matrix(np.array([[-1, 0, -2, -3], [2, 1, 1, 3], [1, 0, 3, 4], [-2, 1, 4, 2]]))\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 1\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0, 1],\n",
       "[0, 1, 0, 0],\n",
       "[0, 0, 1, 1],\n",
       "[0, 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sympy.rref() method  \n",
    "M_rref = M.rref()   \n",
    "      \n",
    "M_rref[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "An **eigenvector** is a vector that for a given transformation, doesn't move off its own span, or in a simple case:\n",
    "\n",
    "- a non-zero vector $\\vec{v}$ associated with a square matrix $A$ such that\n",
    "\n",
    "$$A\\vec{v} = \\lambda\\vec{v}$$\n",
    "\n",
    "In other words, multiplying $\\vec{v}$ by $A$ only scales $\\vec{v}$ by $\\lambda$.\n",
    "\n",
    "- $\\lambda$ is called the **eigenvalue** of the **eigenvector** $\\vec{v}$\n",
    "\n",
    "How would we use this?\n",
    "\n",
    "**Example**\n",
    "\n",
    "Find the eigenvalues and eigenvectors for the given matrix:\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    -1 & 3 \\\\ 0 & 2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We know:\n",
    "\n",
    "$$A\\vec{v} = \\lambda\\vec{v}$$\n",
    "\n",
    "And we wish to find both $\\lambda$ and $\\vec{v}$. We can rewrite:\n",
    "\n",
    "$$A\\vec{v} - \\lambda\\vec{v} = \\vec{0}$$\n",
    "$$(A - \\lambda I)\\vec{v} = \\vec{0}$$\n",
    "\n",
    "How do we determine what the values of $\\lambda$ are? Notice that we want to, essentially, reduce the space of the vector $\\vec{v}$ to the origin. **We want the matrix it is multiplied by to have a determinant of zero**:\n",
    "\n",
    "$$det(A - \\lambda I) = 0$$\n",
    "$$A - \\lambda I = \\begin{bmatrix}\n",
    "    -1-\\lambda & 3 \\\\ 0 & 2-\\lambda\n",
    "\\end{bmatrix}$$\n",
    "$$det(A - \\lambda I) = (-1-\\lambda)(2-\\lambda) - (3)(0) = 0$$\n",
    "$$(-1-\\lambda)(2-\\lambda) = 0$$\n",
    "\n",
    "This implies that the eigenvalues are $\\lambda = \\{-1,2\\}$.\n",
    "\n",
    "And the eigenvectors? Use each of the eigenvalues to figure them out:\n",
    "\n",
    "$$\\lambda = -1$$\n",
    "$$\\begin{bmatrix}\n",
    "    -1-(-1) & 3 \\\\ 0 & 2-1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    x \\\\ y\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    0 \\\\ 0\n",
    "\\end{bmatrix}$$\n",
    "$$\\begin{bmatrix}\n",
    "    0 \\\\ 0\n",
    "\\end{bmatrix}x + \\begin{bmatrix}\n",
    "    3 \\\\ 3\n",
    "\\end{bmatrix}y = \\begin{bmatrix}\n",
    "    0\\\\0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "When $\\lambda=-1$, the eigenvector(s) are anything on the $x$-axis.\n",
    "\n",
    "$$\\lambda = 2$$\n",
    "$$\\begin{bmatrix}\n",
    "    -1-(2) & 3 \\\\ 0 & 2-2\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "    x \\\\ y\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    0 \\\\ 0\n",
    "\\end{bmatrix}$$\n",
    "$$\\begin{bmatrix}\n",
    "    -3 \\\\ 0\n",
    "\\end{bmatrix}x + \\begin{bmatrix}\n",
    "    3 \\\\ 0\n",
    "\\end{bmatrix}y = \\begin{bmatrix}\n",
    "    0\\\\0\n",
    "\\end{bmatrix}$$\n",
    "$$-3x + 3y = 0$$\n",
    "$$-x+y=0$$\n",
    "$$x=y$$\n",
    "\n",
    "When $\\lambda = 2$, the eigenvector(s) are all scaled versions of $\\begin{bmatrix}\n",
    "    1\\\\1\n",
    "\\end{bmatrix}$. For simplicity's sake, we usually just say $\\begin{bmatrix}\n",
    "    1\\\\1\n",
    "\\end{bmatrix}$, since:\n",
    "\n",
    "\n",
    " - **Any scaled version of an eigenvector is also an eigenvector with the same eigenvalue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalues: [-1.  2.]\n"
     ]
    }
   ],
   "source": [
    "# we can use NumPy to find eigenvalue and eigenvector pairs easily\n",
    "A = np.array([[-1, 3], [0, 2]])\n",
    "\n",
    "print('eigenvalues:', np.linalg.eig(A)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1.0 & 0.707106781186547\\\\0 & 0.707106781186547\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1.0, 0.707106781186547],\n",
       "[  0, 0.707106781186547]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format the eigenvector matrix; the columns are the eigenvectors corresponding to the eigenvalues\n",
    "# note that these any multiple of each column is also an eigenvector\n",
    "Matrix(np.linalg.eig(A)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture Break/Practice\n",
    "\n",
    "Find the eigenvalues and corresponding eigenvectors of the matrix:\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "    1 & 5 \\\\ 2 & 4\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Do it first by hand, then use NumPy to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonality\n",
    "\n",
    "Vectors are **orthogonal** if their dot product is zero (equivalently, if the angle between them is 90 degrees).\n",
    "\n",
    "**Examples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the basis vectors are orthogonal\n",
    "np.dot(ihat, jhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "northwest = np.array([-1,1])\n",
    "northeast = np.array([1, 1])\n",
    "np.dot(northwest, northeast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Time: Projections (or, where we start finally approaching Machine Learning)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
